# Adding AI Models to LiteMaaS

This guide shows you how to add AI models to your LiteMaaS deployment and share them with users.

## Two Ways to Add Models

1. **Admin UI** (Recommended) - Easy, web-based configuration
2. **ConfigMap** - Advanced, for pre-configured deployments

---

## Option 1: Using the Admin UI (Easiest)

### Step 1: Access the LiteLLM Admin Portal

After deployment, you'll see:

```
LiteLLM Admin: https://litellm-rhpds.apps.cluster-xxx.opentlc.com
Username: admin
Password: <auto-generated>
```

### Step 2: Login

1. Open the LiteLLM Admin URL
2. Enter the username and password shown in deployment output

### Step 3: Add Models

1. **Navigate to Models** section in the UI
2. **Click "Add Model"**
3. **Fill in the model details:**

#### Example: Adding OpenAI GPT-4

```yaml
Model Name: gpt-4o
Provider: openai/gpt-4o
API Key: sk-your-openai-api-key
```

#### Example: Adding Ollama (Local Model)

```yaml
Model Name: llama3
Provider: ollama/llama3
API Base: http://ollama.example.com:11434
```

#### Example: Adding Azure OpenAI

```yaml
Model Name: azure-gpt-4
Provider: azure/gpt-4
API Key: your-azure-key
API Base: https://your-instance.openai.azure.com/
API Version: 2023-05-15
```

### Step 4: Share with Users

Once models are added:
- Users login to the **User Portal** (Frontend URL)
- Click **"Login with OpenShift"**
- Select available models from dropdown
- Start chatting!

---

## Option 2: Using ConfigMap (Advanced)

For pre-configured deployments, you can create a LiteLLM config file.

### Step 1: Create Configuration File

Create `litellm-config.yaml`:

```yaml
model_list:
  # OpenAI GPT-4o
  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      api_key: "os.environ/OPENAI_API_KEY"

  # Anthropic Claude
  - model_name: claude-3-5-sonnet
    litellm_params:
      model: anthropic/claude-3-5-sonnet-20241022
      api_key: "os.environ/ANTHROPIC_API_KEY"

  # Ollama (local/self-hosted)
  - model_name: llama3
    litellm_params:
      model: ollama/llama3
      api_base: http://ollama-service.ollama.svc.cluster.local:11434

  # Azure OpenAI
  - model_name: azure-gpt-4
    litellm_params:
      model: azure/gpt-4
      api_key: "os.environ/AZURE_API_KEY"
      api_base: "os.environ/AZURE_API_BASE"
      api_version: "2023-05-15"

general_settings:
  master_key: "os.environ/LITELLM_MASTER_KEY"
  database_url: "os.environ/DATABASE_URL"
```

### Step 2: Create Secrets for API Keys

```bash
oc create secret generic litellm-api-keys \
  -n rhpds \
  --from-literal=OPENAI_API_KEY=sk-your-key \
  --from-literal=ANTHROPIC_API_KEY=sk-ant-your-key \
  --from-literal=AZURE_API_KEY=your-azure-key \
  --from-literal=AZURE_API_BASE=https://your-instance.openai.azure.com/
```

### Step 3: Create ConfigMap

```bash
oc create configmap litellm-config \
  -n rhpds \
  --from-file=config.yaml=litellm-config.yaml
```

### Step 4: Update LiteLLM Deployment

```bash
oc set env deployment/litellm -n rhpds \
  --from=secret/litellm-api-keys

oc set volume deployment/litellm -n rhpds \
  --add \
  --name=config-volume \
  --type=configmap \
  --configmap-name=litellm-config \
  --mount-path=/app/config.yaml \
  --sub-path=config.yaml

oc set env deployment/litellm -n rhpds \
  CONFIG_FILE_PATH=/app/config.yaml

oc rollout restart deployment/litellm -n rhpds
```

---

## Supported Model Providers

LiteLLM supports 100+ providers. Here are the most common:

| Provider | Format | Example |
|----------|--------|---------|
| **OpenAI** | `openai/model-name` | `openai/gpt-4o` |
| **Anthropic** | `anthropic/model-name` | `anthropic/claude-3-5-sonnet-20241022` |
| **Azure OpenAI** | `azure/deployment-name` | `azure/gpt-4` |
| **Ollama** | `ollama/model-name` | `ollama/llama3` |
| **AWS Bedrock** | `bedrock/model-id` | `bedrock/anthropic.claude-3-sonnet-20240229-v1:0` |
| **Google Vertex** | `vertex_ai/model-name` | `vertex_ai/gemini-pro` |
| **Hugging Face** | `huggingface/model-path` | `huggingface/meta-llama/Llama-3-8b` |
| **Local Models** | `openai/model-name` with custom api_base | See below |

---

## Common Scenarios

### Scenario 1: Add DeepSeek (Recommended - Best Value)

**Quick Method (UI):**
1. Login to LiteLLM Admin
2. Go to "Add Model"
3. Provider: Select `Deepseek`
4. LiteLLM Model Name: `deepseek/deepseek-chat`
5. Public Name: `DeepSeek Chat`
6. Mode: `Chat - /chat/completions`
7. API Key: Get free key from https://platform.deepseek.com/
8. Click "Add Model" (test may fail, but model will work)

**Users can now:**
- Login to User Portal
- Select "deepseek-chat" from model dropdown
- Start chatting

**Why DeepSeek:**
- ✅ Excellent quality (GPT-4 level)
- ✅ Very affordable ($0.27/1M tokens)
- ✅ Fast responses
- ✅ Free credits on signup

### Scenario 2: Add OpenAI Models

**Quick Method (UI):**
1. Login to LiteLLM Admin
2. Add Model → Name: `gpt-4o`
3. Provider: `openai/gpt-4o`
4. API Key: Your OpenAI key
5. Save

**Users can now:**
- Login to User Portal
- Select "gpt-4o" from model dropdown
- Start chatting

### Scenario 3: Add Self-Hosted Ollama

**Prerequisites:**
- Ollama running in your cluster or accessible endpoint

**Using UI:**
1. Add Model → Name: `llama3`
2. Provider: `ollama/llama3`
3. API Base: `http://ollama-service:11434`
4. No API key needed
5. Save

### Scenario 4: Add Multiple Azure OpenAI Deployments

**Using UI:**

```
Model 1:
  Name: azure-gpt-4-east
  Provider: azure/gpt-4
  API Key: key-for-east-region
  API Base: https://east-instance.openai.azure.com/
  API Version: 2023-05-15

Model 2:
  Name: azure-gpt-4-west
  Provider: azure/gpt-4
  API Key: key-for-west-region
  API Base: https://west-instance.openai.azure.com/
  API Version: 2023-05-15
```

LiteLLM will automatically load balance between them.

### Scenario 5: Add Red Hat OpenShift AI Models

**Prerequisites:**
- OpenShift AI deployed with model serving
- Model inference endpoint available

**Using UI:**
1. Add Model → Name: `granite-7b`
2. Provider: `openai/granite-7b-instruct`
3. API Base: `https://granite-inference-route.apps.cluster.com/v1`
4. API Key: Your RHOAI token
5. Save

---

## User Access Management

### Two-Tier Access Model

**Admin Users:**
- Access LiteLLM Admin Portal (`https://litellm-rhpds.apps...`)
- Add/remove AI models
- Create virtual keys for users
- View usage and costs
- Monitor spending

**Regular Users:**
- Receive virtual keys from admins
- Access models via API/SDK only
- Cannot manage models or view admin portal

### Creating Virtual Keys for Users

#### Via LiteLLM Admin UI (Recommended)

1. **Login to LiteLLM Admin**
2. **Click "Virtual Keys"** in the sidebar
3. **Click "Generate Key"** or "Add Key"
4. **Fill in:**
   ```
   Models: Select models (e.g., "deepseek-chat")
   User ID: user@example.com (optional)
   Max Budget: 100 (dollars, optional)
   Duration: 30d (optional, key expiration)
   ```
5. **Copy the generated key** (starts with `sk-`)
6. **Share key with user**

#### Via API

```bash
# Get LiteLLM master key and URL
LITELLM_MASTER_KEY=$(oc get secret litellm-secret -n rhpds -o jsonpath='{.data.LITELLM_MASTER_KEY}' | base64 -d)
LITELLM_URL=$(oc get route litellm -n rhpds -o jsonpath='{.spec.host}')

# Create virtual key for a user
curl "https://${LITELLM_URL}/key/generate" \
  -H "Authorization: Bearer ${LITELLM_MASTER_KEY}" \
  -H "Content-Type: application/json" \
  -d '{
    "models": ["deepseek-chat"],
    "max_budget": 100,
    "duration": "30d",
    "metadata": {
      "user_email": "user@example.com",
      "description": "User access key"
    }
  }'
```

Response:
```json
{
  "key": "sk-xxxxxxxxxxxxxx",
  "expires": "2025-12-05T00:00:00"
}
```

#### Using the Helper Script

```bash
# Create a virtual key
./scripts/create-user-key.sh user@example.com deepseek-chat 100 30d
```

### How Users Access Models

Once users have a virtual key, they can access models via API:

#### Using curl

```bash
curl https://litellm-rhpds.apps.your-cluster.com/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-user-virtual-key" \
  -d '{
    "model": "deepseek-chat",
    "messages": [
      {"role": "user", "content": "Hello! What is AI?"}
    ]
  }'
```

#### Using Python SDK

```python
import openai

client = openai.OpenAI(
    api_key="sk-user-virtual-key",
    base_url="https://litellm-rhpds.apps.your-cluster.com"
)

response = client.chat.completions.create(
    model="deepseek-chat",
    messages=[{"role": "user", "content": "Hello!"}]
)

print(response.choices[0].message.content)
```

#### Using Node.js

```javascript
const OpenAI = require('openai');

const client = new OpenAI({
  apiKey: 'sk-user-virtual-key',
  baseURL: 'https://litellm-rhpds.apps.your-cluster.com'
});

async function main() {
  const response = await client.chat.completions.create({
    model: 'deepseek-chat',
    messages: [{ role: 'user', content: 'Hello!' }]
  });

  console.log(response.choices[0].message.content);
}

main();
```

### Key Management Best Practices

1. **Set Budget Limits**: Always set `max_budget` to prevent overspending
2. **Set Expiration**: Use `duration` parameter (e.g., "30d", "90d")
3. **Model Restrictions**: Only grant access to needed models
4. **Track Usage**: Monitor spending in LiteLLM Admin
5. **Rotate Keys**: Regenerate keys periodically
6. **Revoke Access**: Delete or block keys when users leave

### Monitoring User Usage

Check spending and usage:

```bash
# Check key info
curl "https://${LITELLM_URL}/key/info?key=sk-user-key" \
  -H "Authorization: Bearer ${LITELLM_MASTER_KEY}"

# Check all keys
curl "https://${LITELLM_URL}/key/info" \
  -H "Authorization: Bearer ${LITELLM_MASTER_KEY}"
```

### Revoking User Access

Block a virtual key:

```bash
curl "https://${LITELLM_URL}/key/block" \
  -X POST \
  -H "Authorization: Bearer ${LITELLM_MASTER_KEY}" \
  -H "Content-Type: application/json" \
  -d '{"key": "sk-key-to-block"}'
```

Or delete permanently via the Admin UI.

---

## Checking Model Status

### Via UI

1. Login to LiteLLM Admin
2. Navigate to **Models** section
3. See all configured models and their status

### Via CLI

```bash
# Get LiteLLM route
LITELLM_URL=$(oc get route litellm -n rhpds -o jsonpath='{.spec.host}')

# List models
curl -X GET "https://${LITELLM_URL}/model/info" \
  -H "Authorization: Bearer ${LITELLM_MASTER_KEY}"
```

---

## Troubleshooting

### Model Not Showing Up

1. **Check LiteLLM logs:**
   ```bash
   oc logs -n rhpds deployment/litellm --tail=100
   ```

2. **Verify API key is correct:**
   ```bash
   oc get secret litellm-api-keys -n rhpds -o yaml
   ```

3. **Test model directly:**
   ```bash
   curl https://${LITELLM_URL}/chat/completions \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer ${LITELLM_MASTER_KEY}" \
     -d '{
       "model": "gpt-4o",
       "messages": [{"role": "user", "content": "Hello"}]
     }'
   ```

### Users Can't See New Models

1. **Refresh browser** (hard refresh: Cmd+Shift+R or Ctrl+Shift+F5)
2. **Check backend logs:**
   ```bash
   oc logs -n rhpds deployment/litemaas-backend --tail=100
   ```
3. **Verify backend can reach LiteLLM:**
   ```bash
   oc exec -n rhpds deployment/litemaas-backend -- curl http://litellm:4000/health
   ```

### Rate Limiting Issues

If you hit rate limits, configure RPM (requests per minute) in your model config:

```yaml
model_list:
  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      api_key: sk-your-key
      rpm: 60  # Limit to 60 requests per minute
```

---

## Best Practices

1. **Use environment variables** for API keys (never hardcode)
2. **Set rate limits** to avoid provider throttling
3. **Test models** using LiteLLM admin before sharing with users
4. **Monitor costs** in LiteLLM admin dashboard
5. **Use load balancing** for production workloads (multiple deployments of same model)
6. **Enable caching** for frequently asked questions

---

## Next Steps

- [LiteLLM Supported Providers](https://docs.litellm.ai/docs/providers)
- [LiteLLM Load Balancing](https://docs.litellm.ai/docs/proxy/load_balancing)
- [Cost Tracking](https://docs.litellm.ai/docs/proxy/cost_tracking)
- [Caching Configuration](https://docs.litellm.ai/docs/proxy/caching)
